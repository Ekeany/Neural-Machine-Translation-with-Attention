{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"theActualNLP.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"jzcap7jAWBVE","colab_type":"code","outputId":"f10fa725-c896-48af-a7b4-9934fe9c73f8","executionInfo":{"status":"ok","timestamp":1554808973175,"user_tz":-60,"elapsed":24994,"user":{"displayName":"Eoghan Keany","photoUrl":"","userId":"01744055321449573695"}},"colab":{"base_uri":"https://localhost:8080/","height":173}},"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from google.colab import files\n","import string\n","import io\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","from functools import reduce\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from nltk.stem import PorterStemmer\n","import math\n","import re\n","import seaborn as sns\n","from copy import copy, deepcopy\n","from unicodedata import normalize\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from keras.layers import Embedding\n","from keras.preprocessing.text import one_hot\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from keras.layers.embeddings import Embedding\n","from numpy import asarray\n","\n","# Load the Drive helper and mount\n","from google.colab import drive\n","\n","# This will prompt for authorization.\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"metadata":{"id":"6m6I2rAAWJxo","colab_type":"code","colab":{}},"cell_type":"code","source":["file = open('drive/My Drive/spa.txt').read().split('\\n')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zIOqn38sWPyY","colab_type":"code","outputId":"f30d8a19-6aff-49d5-f2ef-ffee107b72b5","executionInfo":{"status":"ok","timestamp":1554809001972,"user_tz":-60,"elapsed":872,"user":{"displayName":"Eoghan Keany","photoUrl":"","userId":"01744055321449573695"}},"colab":{"base_uri":"https://localhost:8080/","height":1105}},"cell_type":"code","source":["def My_train_test_split(df,df2,train_percent, test_percent):\n","    # calculate random permutation of indices\n","    m = len(df)\n","    perm = np.random.permutation(m)\n","    train_end = int(train_percent * m)\n","    test_end = int(test_percent * m) + train_end\n","    \n","    Xtrain = [df[i] for i in perm[:train_end]]\n","    Xtest = [df[i] for i in perm[train_end:test_end]]\n","    \n","    Ytrain = [df2[i] for i in perm[:train_end]]\n","    Ytest = [df2[i] for i in perm[train_end:test_end]]\n","    return(Xtrain, Xtest, Ytrain,Ytest,)  \n","  \n","def clean_pairs(lines):\n","  cleaned = list()\n","  # prepare regex for char filtering\n","  re_print = re.compile('[^%s]' % re.escape(string.printable))\n","  # prepare translation table for removing punctuation\n","  table = str.maketrans('', '', string.punctuation)\n","  for pair in lines:\n","    clean_pair = list()\n","    for line in pair:\n","      # rmove word characters.\n","      line = normalize('NFD', line).encode('ascii', 'ignore')\n","      line = line.decode('UTF-8')\n","      # tokenize on white space\n","      line = line.split()\n","      # convert to lowercase\n","      line = [word.lower() for word in line]\n","      # remove punctuation from each token\n","      line = [word.translate(table) for word in line]\n","      # remove non-printable chars form each token\n","      line = [w.strip() for w in line]\n","      # remove tokens with numbers in them\n","      line = [word for word in line if word.isalpha()]\n","      # store as string\n","      clean_pair.append(' '.join(line))\n","    cleaned.append(clean_pair)\n","  return(cleaned)\n","\n","lines = file[0:7500]\n","split = [re.split(r'\\t+', item)for item in lines]\n","split = clean_pairs(split)\n","X  = [x[0] for x in split]\n","Y  = ['START_ ' + x[1] + ' _END'for x in split]\n","#Y = [y.split() for y in Y]\n","\n","df = pd.DataFrame(\n","    {'X': X,\n","     'Y': Y,\n","    })\n","\n","df = df.drop_duplicates(subset=['X'], keep= 'first')\n","\n","print(df)\n","X = df.X.values\n","X  = [x.split() for x in X]\n","Y = df.Y.values\n","Y  = [y.split() for y in Y]\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["                    X                                      Y\n","0                  go                         START_ ve _END\n","4                  hi                       START_ hola _END\n","5                 run                      START_ corre _END\n","7                 who                      START_ quien _END\n","8                 wow                      START_ orale _END\n","9                fire                      START_ fuego _END\n","12               help                      START_ ayuda _END\n","15               jump                      START_ salta _END\n","17               stop                      START_ parad _END\n","20               wait                     START_ espera _END\n","22              go on                   START_ continua _END\n","24              hello                       START_ hola _END\n","25              i ran                      START_ corri _END\n","27              i try                 START_ lo intento _END\n","28              i won                  START_ he ganado _END\n","29              oh no                      START_ oh no _END\n","30              relax          START_ tomatelo con soda _END\n","31              smile                     START_ sonrie _END\n","32             attack                  START_ al ataque _END\n","34             get up                    START_ levanta _END\n","35             go now             START_ ve ahora mismo _END\n","36             got it                   START_ lo tengo _END\n","39             he ran                  START_ el corrio _END\n","40             hop in             START_ metete adentro _END\n","41             hug me                   START_ abrazame _END\n","42             i fell                     START_ me cai _END\n","43             i know                   START_ yo lo se _END\n","44             i left                       START_ sali _END\n","45             i lied                      START_ menti _END\n","46             i lost                      START_ perdi _END\n","...               ...                                    ...\n","7455    youre in luck           START_ andas con suerte _END\n","7457    youre invited             START_ estas invitado _END\n","7458    youre kidding            START_ estas bromeando _END\n","7459    youre like me               START_ eres como yo _END\n","7462    youre my hero              START_ eres mi heroe _END\n","7464    youre my type               START_ eres mi tipo _END\n","7465    youre not god                START_ no sos dios _END\n","7466    youre not fat               START_ no sos gordo _END\n","7467    youre not old           START_ vos no sos viejo _END\n","7468    youre on time             START_ estas a tiempo _END\n","7469    youre patient               START_ sos paciente _END\n","7471    youre psyched          START_ estas mentalizado _END\n","7472    youre shallow        START_ tu eres superficial _END\n","7473    youre sloshed             START_ estas borracho _END\n","7475    youre so thin              START_ sos muy flaco _END\n","7476    youre useless             START_ sos inservible _END\n","7483    youre winning              START_ estas ganando _END\n","7485    youre wounded               START_ estas herida _END\n","7486    youve changed               START_ has cambiado _END\n","7487    youve done it               START_ lo has hecho _END\n","7489   yours is worse            START_ el tuyo es peor _END\n","7491  a boat capsized            START_ un bote zozobro _END\n","7492  a fish can swim         START_ un pez puede nadar _END\n","7493  a man must work    START_ un hombre debe trabajar _END\n","7494  about what time     START_ a que hora mas o menos _END\n","7495  all are present      START_ todos estan presentes _END\n","7496  all were silent  START_ todos estaban en silencio _END\n","7497  am i alone here            START_ estoy solo aqui _END\n","7498  am i boring you        START_ te estoy aburriendo _END\n","7499  am i in trouble         START_ estoy en problemas _END\n","\n","[5308 rows x 2 columns]\n"],"name":"stdout"}]},{"metadata":{"id":"V4ovc1FOWl1q","colab_type":"code","colab":{}},"cell_type":"code","source":["all_eng_words  = set([item for sublist in X for item in sublist])\n","all_spa_words = set([item for sublist in Y for item in sublist])\n","\n","eng_sentence_length = max([len(sample.split()) for sample in df.X])\n","spa_sentence_length = max([len(sample.split()) for sample in df.Y])\n","\n","input_words = sorted(list(all_eng_words))\n","target_words = sorted(list(all_spa_words))\n","num_encoder_tokens = len(all_eng_words)\n","num_decoder_tokens = len(all_spa_words)\n","\n","input_token_index = dict(\n","    [(word, i) for i, word in enumerate(input_words)])\n","target_token_index = dict(\n","    [(word, i) for i, word in enumerate(target_words)])\n","\n","encoder_input_data = np.zeros(\n","    (len(df.X), eng_sentence_length),\n","    dtype='float32')\n","decoder_input_data = np.zeros(\n","    (len(df.Y), spa_sentence_length),\n","    dtype='float32')\n","decoder_target_data = np.zeros(\n","    (len(df.Y), spa_sentence_length, num_decoder_tokens),\n","    dtype='float32')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LsCzHQpsZDlk","colab_type":"code","colab":{}},"cell_type":"code","source":["for i, (input_text, target_text) in enumerate(zip(df.X, df.Y)):\n","    for t, word in enumerate(input_text.split()):\n","        encoder_input_data[i, t] = input_token_index[word]\n","    for t, word in enumerate(target_text.split()):\n","        # decoder_target_data is ahead of decoder_input_data by one timestep\n","        decoder_input_data[i, t] = target_token_index[word]\n","        if t > 0:\n","            # decoder_target_data will be ahead by one timestep\n","            # and will not include the start character.\n","            decoder_target_data[i, t - 1, target_token_index[word]] = 1.\n","            \n","            \n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"prj7R-SFhzV1","colab_type":"code","colab":{}},"cell_type":"code","source":["embeddings_index = dict()\n","f = open('drive/My Drive/glove.6B.100d.txt').readlines()\n","for line in f:\n","\tvalues = line.split()\n","\tword = values[0]\n","\tcoefs = np.asarray(values[1:], dtype='float32')\n","\tembeddings_index[word] = coefs\n","\n","MAX_NUM_WORDS = 1880\n","EMBEDDING_DIM = embeddings_index.get('a').shape[0]\n","num_words = min(MAX_NUM_WORDS, len(all_eng_words)) + 1\n","embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n","for i, word in enumerate(list(all_eng_words)):\n","    if i > MAX_NUM_WORDS:\n","        continue\n","    embedding_vector = embeddings_index.get(word) ## This references the loaded embeddings dictionary\n","    if embedding_vector is not None:\n","        # words not found in embedding index will be all-zeros.\n","        embedding_matrix[i] = embedding_vector\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"iVoVqBBSjjvv","colab_type":"code","outputId":"8c95e046-f674-47ed-c7c1-0382c993e792","executionInfo":{"status":"ok","timestamp":1554809091141,"user_tz":-60,"elapsed":625,"user":{"displayName":"Eoghan Keany","photoUrl":"","userId":"01744055321449573695"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["embedding_layer = Embedding(num_encoder_tokens,50,weights=[embedding_matrix],input_length=eng_sentence_length)\n","print(len(embedding_matrix))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["1881\n"],"name":"stdout"}]},{"metadata":{"id":"jmXtcjXla9Ui","colab_type":"code","outputId":"884d3391-e97b-4a92-a3f1-6ff869951315","executionInfo":{"status":"ok","timestamp":1554809094072,"user_tz":-60,"elapsed":1162,"user":{"displayName":"Eoghan Keany","photoUrl":"","userId":"01744055321449573695"}},"colab":{"base_uri":"https://localhost:8080/","height":408}},"cell_type":"code","source":["from keras.layers import Input, LSTM, Embedding, Dense\n","from keras.models import Model\n","from keras.utils import plot_model\n","\n","embedding_size = 100\n","encoder_inputs = Input(shape=(None,))\n","#en_x=  Embedding(num_encoder_tokens, embedding_size)(encoder_inputs)\n","en_x = Embedding(num_encoder_tokens, embedding_size, weights= [embedding_matrix], trainable=False)(encoder_inputs)\n","encoder = LSTM(50, return_state=True)\n","encoder_outputs, state_h, state_c = encoder(en_x)\n","# We discard `encoder_outputs` and only keep the states.\n","encoder_states = [state_h, state_c]\n","\n","\n","# Set up the decoder, using `encoder_states` as initial state.\n","decoder_inputs = Input(shape=(None,))\n","dex=  Embedding(num_decoder_tokens, embedding_size)\n","final_dex= dex(decoder_inputs)\n","decoder_lstm = LSTM(50, return_sequences=True, return_state=True)\n","decoder_outputs, _, _ = decoder_lstm(final_dex,\n","                                     initial_state=encoder_states)\n","decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n","decoder_outputs = decoder_dense(decoder_outputs)\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n","model.summary()"],"execution_count":13,"outputs":[{"output_type":"stream","text":["__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_2 (InputLayer)            (None, None)         0                                            \n","__________________________________________________________________________________________________\n","input_3 (InputLayer)            (None, None)         0                                            \n","__________________________________________________________________________________________________\n","embedding_5 (Embedding)         (None, None, 100)    188100      input_2[0][0]                    \n","__________________________________________________________________________________________________\n","embedding_6 (Embedding)         (None, None, 100)    293600      input_3[0][0]                    \n","__________________________________________________________________________________________________\n","lstm_1 (LSTM)                   [(None, 50), (None,  30200       embedding_5[0][0]                \n","__________________________________________________________________________________________________\n","lstm_2 (LSTM)                   [(None, None, 50), ( 30200       embedding_6[0][0]                \n","                                                                 lstm_1[0][1]                     \n","                                                                 lstm_1[0][2]                     \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, None, 2936)   149736      lstm_2[0][0]                     \n","==================================================================================================\n","Total params: 691,836\n","Trainable params: 503,736\n","Non-trainable params: 188,100\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"metadata":{"id":"vkNzJrNPbV27","colab_type":"code","outputId":"f811d7e6-d387-46a2-a88d-a9eeeca434ee","executionInfo":{"status":"ok","timestamp":1554809616048,"user_tz":-60,"elapsed":514196,"user":{"displayName":"Eoghan Keany","photoUrl":"","userId":"01744055321449573695"}},"colab":{"base_uri":"https://localhost:8080/","height":1873}},"cell_type":"code","source":["model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n","          batch_size=128,\n","          epochs=50,\n","          validation_split=0.05)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Deprecated in favor of operator or tf.math.divide.\n","Train on 5042 samples, validate on 266 samples\n","Epoch 1/50\n","5042/5042 [==============================] - 12s 2ms/step - loss: 2.5206 - acc: 0.0665 - val_loss: 2.3011 - val_acc: 0.1000\n","Epoch 2/50\n","5042/5042 [==============================] - 10s 2ms/step - loss: 2.0129 - acc: 0.1000 - val_loss: 2.1610 - val_acc: 0.1000\n","Epoch 3/50\n","5042/5042 [==============================] - 10s 2ms/step - loss: 1.9115 - acc: 0.1000 - val_loss: 2.1353 - val_acc: 0.1000\n","Epoch 4/50\n","5042/5042 [==============================] - 10s 2ms/step - loss: 1.8598 - acc: 0.1000 - val_loss: 2.1158 - val_acc: 0.1000\n","Epoch 5/50\n","5042/5042 [==============================] - 10s 2ms/step - loss: 1.8050 - acc: 0.1018 - val_loss: 2.0821 - val_acc: 0.1132\n","Epoch 6/50\n","5042/5042 [==============================] - 10s 2ms/step - loss: 1.7533 - acc: 0.1094 - val_loss: 2.0509 - val_acc: 0.1132\n","Epoch 7/50\n","5042/5042 [==============================] - 10s 2ms/step - loss: 1.7078 - acc: 0.1104 - val_loss: 2.0390 - val_acc: 0.1147\n","Epoch 8/50\n","5042/5042 [==============================] - 10s 2ms/step - loss: 1.6650 - acc: 0.1162 - val_loss: 1.9874 - val_acc: 0.1169\n","Epoch 9/50\n","5042/5042 [==============================] - 10s 2ms/step - loss: 1.6210 - acc: 0.1226 - val_loss: 1.9834 - val_acc: 0.1203\n","Epoch 10/50\n","5042/5042 [==============================] - 10s 2ms/step - loss: 1.5803 - acc: 0.1286 - val_loss: 1.9533 - val_acc: 0.1218\n","Epoch 11/50\n","5042/5042 [==============================] - 10s 2ms/step - loss: 1.5428 - acc: 0.1332 - val_loss: 1.9539 - val_acc: 0.1211\n","Epoch 12/50\n","5042/5042 [==============================] - 10s 2ms/step - loss: 1.5091 - acc: 0.1377 - val_loss: 1.9160 - val_acc: 0.1278\n","Epoch 13/50\n","5042/5042 [==============================] - 10s 2ms/step - loss: 1.4773 - acc: 0.1414 - val_loss: 1.9100 - val_acc: 0.1267\n","Epoch 14/50\n","5042/5042 [==============================] - 10s 2ms/step - loss: 1.4485 - acc: 0.1447 - val_loss: 1.8947 - val_acc: 0.1293\n","Epoch 15/50\n","5042/5042 [==============================] - 10s 2ms/step - loss: 1.4219 - acc: 0.1473 - val_loss: 1.8904 - val_acc: 0.1342\n","Epoch 16/50\n","5042/5042 [==============================] - 10s 2ms/step - loss: 1.3959 - acc: 0.1501 - val_loss: 1.8901 - val_acc: 0.1323\n","Epoch 17/50\n","5042/5042 [==============================] - 10s 2ms/step - loss: 1.3717 - acc: 0.1520 - val_loss: 1.8537 - val_acc: 0.1376\n","Epoch 18/50\n","5042/5042 [==============================] - 10s 2ms/step - loss: 1.3480 - acc: 0.1534 - val_loss: 1.8559 - val_acc: 0.1391\n","Epoch 19/50\n","5042/5042 [==============================] - 10s 2ms/step - loss: 1.3261 - acc: 0.1564 - val_loss: 1.8308 - val_acc: 0.1406\n","Epoch 20/50\n","5042/5042 [==============================] - 10s 2ms/step - loss: 1.3036 - acc: 0.1585 - val_loss: 1.8304 - val_acc: 0.1414\n","Epoch 21/50\n","5042/5042 [==============================] - 10s 2ms/step - loss: 1.2832 - acc: 0.1610 - val_loss: 1.8196 - val_acc: 0.1440\n","Epoch 22/50\n","5042/5042 [==============================] - 10s 2ms/step - loss: 1.2625 - acc: 0.1628 - val_loss: 1.8021 - val_acc: 0.1504\n","Epoch 23/50\n","5042/5042 [==============================] - 10s 2ms/step - loss: 1.2427 - acc: 0.1662 - val_loss: 1.8028 - val_acc: 0.1500\n","Epoch 24/50\n","5042/5042 [==============================] - 10s 2ms/step - loss: 1.2226 - acc: 0.1682 - val_loss: 1.8142 - val_acc: 0.1459\n","Epoch 25/50\n","5042/5042 [==============================] - 10s 2ms/step - loss: 1.2052 - acc: 0.1708 - val_loss: 1.7772 - val_acc: 0.1477\n","Epoch 26/50\n","5042/5042 [==============================] - 10s 2ms/step - loss: 1.1864 - acc: 0.1724 - val_loss: 1.7739 - val_acc: 0.1515\n","Epoch 27/50\n","5042/5042 [==============================] - 10s 2ms/step - loss: 1.1691 - acc: 0.1749 - val_loss: 1.7673 - val_acc: 0.1515\n","Epoch 28/50\n","5042/5042 [==============================] - 10s 2ms/step - loss: 1.1523 - acc: 0.1768 - val_loss: 1.7577 - val_acc: 0.1519\n","Epoch 29/50\n","5042/5042 [==============================] - 10s 2ms/step - loss: 1.1356 - acc: 0.1789 - val_loss: 1.7533 - val_acc: 0.1508\n","Epoch 30/50\n","5042/5042 [==============================] - 10s 2ms/step - loss: 1.1197 - acc: 0.1802 - val_loss: 1.7465 - val_acc: 0.1530\n","Epoch 31/50\n","5042/5042 [==============================] - 10s 2ms/step - loss: 1.1052 - acc: 0.1810 - val_loss: 1.7453 - val_acc: 0.1530\n","Epoch 32/50\n","5042/5042 [==============================] - 10s 2ms/step - loss: 1.0892 - acc: 0.1829 - val_loss: 1.7482 - val_acc: 0.1526\n","Epoch 33/50\n","5042/5042 [==============================] - 10s 2ms/step - loss: 1.0749 - acc: 0.1844 - val_loss: 1.7436 - val_acc: 0.1556\n","Epoch 34/50\n","5042/5042 [==============================] - 10s 2ms/step - loss: 1.0598 - acc: 0.1856 - val_loss: 1.7319 - val_acc: 0.1564\n","Epoch 35/50\n","5042/5042 [==============================] - 10s 2ms/step - loss: 1.0474 - acc: 0.1871 - val_loss: 1.7357 - val_acc: 0.1556\n","Epoch 36/50\n","5042/5042 [==============================] - 10s 2ms/step - loss: 1.0336 - acc: 0.1885 - val_loss: 1.7307 - val_acc: 0.1568\n","Epoch 37/50\n","5042/5042 [==============================] - 10s 2ms/step - loss: 1.0197 - acc: 0.1898 - val_loss: 1.7310 - val_acc: 0.1579\n","Epoch 38/50\n","5042/5042 [==============================] - 10s 2ms/step - loss: 1.0069 - acc: 0.1915 - val_loss: 1.7256 - val_acc: 0.1583\n","Epoch 39/50\n","5042/5042 [==============================] - 11s 2ms/step - loss: 0.9944 - acc: 0.1919 - val_loss: 1.7229 - val_acc: 0.1594\n","Epoch 40/50\n","5042/5042 [==============================] - 10s 2ms/step - loss: 0.9831 - acc: 0.1934 - val_loss: 1.7413 - val_acc: 0.1579\n","Epoch 41/50\n","5042/5042 [==============================] - 10s 2ms/step - loss: 0.9696 - acc: 0.1947 - val_loss: 1.7304 - val_acc: 0.1568\n","Epoch 42/50\n","5042/5042 [==============================] - 10s 2ms/step - loss: 0.9576 - acc: 0.1967 - val_loss: 1.7272 - val_acc: 0.1602\n","Epoch 43/50\n","5042/5042 [==============================] - 10s 2ms/step - loss: 0.9454 - acc: 0.1973 - val_loss: 1.7374 - val_acc: 0.1541\n","Epoch 44/50\n","5042/5042 [==============================] - 10s 2ms/step - loss: 0.9344 - acc: 0.1987 - val_loss: 1.7285 - val_acc: 0.1609\n","Epoch 45/50\n","5042/5042 [==============================] - 10s 2ms/step - loss: 0.9241 - acc: 0.2004 - val_loss: 1.7303 - val_acc: 0.1568\n","Epoch 46/50\n","5042/5042 [==============================] - 10s 2ms/step - loss: 0.9123 - acc: 0.2019 - val_loss: 1.7321 - val_acc: 0.1602\n","Epoch 47/50\n","5042/5042 [==============================] - 10s 2ms/step - loss: 0.9020 - acc: 0.2024 - val_loss: 1.7273 - val_acc: 0.1579\n","Epoch 48/50\n","5042/5042 [==============================] - 10s 2ms/step - loss: 0.8903 - acc: 0.2037 - val_loss: 1.7331 - val_acc: 0.1564\n","Epoch 49/50\n","5042/5042 [==============================] - 10s 2ms/step - loss: 0.8810 - acc: 0.2045 - val_loss: 1.7307 - val_acc: 0.1590\n","Epoch 50/50\n","5042/5042 [==============================] - 10s 2ms/step - loss: 0.8697 - acc: 0.2057 - val_loss: 1.7439 - val_acc: 0.1541\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f37f52a3a58>"]},"metadata":{"tags":[]},"execution_count":14}]},{"metadata":{"id":"jY5AVB3_cFDT","colab_type":"code","outputId":"f404956c-16c3-4cce-b2af-af9ebe3964e3","executionInfo":{"status":"ok","timestamp":1554809738453,"user_tz":-60,"elapsed":727,"user":{"displayName":"Eoghan Keany","photoUrl":"","userId":"01744055321449573695"}},"colab":{"base_uri":"https://localhost:8080/","height":238}},"cell_type":"code","source":["encoder_model = Model(encoder_inputs, encoder_states)\n","encoder_model.summary()"],"execution_count":18,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_2 (InputLayer)         (None, None)              0         \n","_________________________________________________________________\n","embedding_5 (Embedding)      (None, None, 100)         188100    \n","_________________________________________________________________\n","lstm_1 (LSTM)                [(None, 50), (None, 50),  30200     \n","=================================================================\n","Total params: 218,300\n","Trainable params: 30,200\n","Non-trainable params: 188,100\n","_________________________________________________________________\n"],"name":"stdout"}]},{"metadata":{"id":"SNd79vdJcL0A","colab_type":"code","colab":{}},"cell_type":"code","source":["decoder_state_input_h = Input(shape=(50,))\n","decoder_state_input_c = Input(shape=(50,))\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","\n","final_dex2= dex(decoder_inputs)\n","\n","decoder_outputs2, state_h2, state_c2 = decoder_lstm(final_dex2, initial_state=decoder_states_inputs)\n","decoder_states2 = [state_h2, state_c2]\n","decoder_outputs2 = decoder_dense(decoder_outputs2)\n","decoder_model = Model(\n","    [decoder_inputs] + decoder_states_inputs,\n","    [decoder_outputs2] + decoder_states2)\n","\n","# Reverse-lookup token index to decode sequences back to\n","# something readable.\n","reverse_input_char_index = dict(\n","    (i, char) for char, i in input_token_index.items())\n","reverse_target_char_index = dict(\n","    (i, char) for char, i in target_token_index.items())"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OsHq8jJ3cpgB","colab_type":"code","colab":{}},"cell_type":"code","source":["def decode_sequence(input_seq):\n","    # Encode the input as state vectors.\n","    states_value = encoder_model.predict(input_seq)\n","    # Generate empty target sequence of length 1.\n","    target_seq = np.zeros((1,1))\n","    # Populate the first character of target sequence with the start character.\n","    target_seq[0, 0] = target_token_index['START_']\n","\n","    # Sampling loop for a batch of sequences\n","    # (to simplify, here we assume a batch of size 1).\n","    stop_condition = False\n","    decoded_sentence = ''\n","    while not stop_condition:\n","        output_tokens, h, c = decoder_model.predict(\n","            [target_seq] + states_value)\n","\n","        # Sample a token\n","        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","        sampled_char = reverse_target_char_index[sampled_token_index]\n","        decoded_sentence += ' '+sampled_char\n","\n","        # Exit condition: either hit max length\n","        # or find stop character.\n","        if (sampled_char == '_END' or\n","           len(decoded_sentence) > 52):\n","            stop_condition = True\n","\n","        # Update the target sequence (of length 1).\n","        target_seq = np.zeros((1,1))\n","        target_seq[0, 0] = sampled_token_index\n","\n","        # Update states\n","        states_value = [h, c]\n","\n","    return decoded_sentence"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rdYWlY3ddA3B","colab_type":"code","outputId":"581dbff0-3a03-49de-9eac-7b02de4218cf","executionInfo":{"status":"ok","timestamp":1554809743295,"user_tz":-60,"elapsed":808,"user":{"displayName":"Eoghan Keany","photoUrl":"","userId":"01744055321449573695"}},"colab":{"base_uri":"https://localhost:8080/","height":697}},"cell_type":"code","source":["from nltk.translate.bleu_score import sentence_bleu\n","\n","for seq_index in [30,22,35,500, 700, 800,5000,4500,3000,2000]:\n","    input_seq = encoder_input_data[seq_index: seq_index + 1]\n","    decoded_sentence = decode_sequence(input_seq)\n","    print('-')\n","    print('Input sentence:', df.X[seq_index: seq_index + 1])\n","    print('Decoded sentence:', decoded_sentence)\n"],"execution_count":19,"outputs":[{"output_type":"stream","text":["-\n","Input sentence: 47    i quit\n","Name: X, dtype: object\n","Decoded sentence:  tom se se quedo _END\n","-\n","Input sentence: 39    he ran\n","Name: X, dtype: object\n","Decoded sentence:  el se quedo _END\n","-\n","Input sentence: 53    listen\n","Name: X, dtype: object\n","Decoded sentence:  son caja _END\n","-\n","Input sentence: 850    wait here\n","Name: X, dtype: object\n","Decoded sentence:  aqui aqui _END\n","-\n","Input sentence: 1157    im better\n","Name: X, dtype: object\n","Decoded sentence:  estoy cansado _END\n","-\n","Input sentence: 1314    quiet down\n","Name: X, dtype: object\n","Decoded sentence:  vuelve a casa _END\n","-\n","Input sentence: 7116    tom is useless\n","Name: X, dtype: object\n","Decoded sentence:  tom es tom _END\n","-\n","Input sentence: 6440    it looks great\n","Name: X, dtype: object\n","Decoded sentence:  se es bien _END\n","-\n","Input sentence: 4453    i said enough\n","Name: X, dtype: object\n","Decoded sentence:  yo dije que dije _END\n","-\n","Input sentence: 3040    ive lost it\n","Name: X, dtype: object\n","Decoded sentence:  lo lo he hecho _END\n"],"name":"stdout"}]},{"metadata":{"id":"w7X47EbfEjLv","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n","from keras.layers import RepeatVector, Dense, Activation, Lambda\n","from keras.optimizers import Adam\n","from keras.utils import to_categorical\n","from keras.models import load_model, Model\n","import keras.backend as K\n","import numpy as np\n","\n","def softmax(x):\n","  e = K.exp(x - K.max(x, axis=1, keepdims=True))\n","  s = K.sum(e, axis=1, keepdims=True)\n","  return e / s\n","\n","\n","repeator = RepeatVector(eng_sentence_length)\n","concatenator = Concatenate(axis=-1)\n","densor = Dense(1, activation = \"relu\")\n","activator = Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n","dotor = Dot(axes = 1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wGPpQWeVGBsA","colab_type":"code","colab":{}},"cell_type":"code","source":["def one_step_attention(a, s_prev):    \n","    ### START CODE HERE ###\n","    # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states \"a\" (≈ 1 line)\n","    s_prev = repeator(s_prev)\n","    # Use concatenator to concatenate a and s_prev on the last axis (≈ 1 line)\n","    concat = concatenator([a, s_prev])\n","    # Use densor to propagate concat through a small fully-connected neural network to compute the \"energies\" variable e. (≈1 lines)\n","    e = densor(concat)\n","    # Use activator and e to compute the attention weights \"alphas\" (≈ 1 line)\n","    alphas = activator(e)\n","    # Use dotor together with \"alphas\" and \"a\" to compute the context vector to be given to the next (post-attention) LSTM-cell (≈ 1 line)\n","    context = dotor([alphas, a])\n","    ### END CODE HERE ###  \n","    return context\n","  \n","  \n","n_a = 64\n","n_s = 128\n","post_activation_LSTM_cell = LSTM(n_s, return_state = True)\n","output_layer = Dense(num_decoder_tokens, activation=softmax)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"AIh7likjI5AI","colab_type":"code","colab":{}},"cell_type":"code","source":["def model(Tx, Ty, n_a,n_s, human_vocab_size, machine_vocab_size):\n","    # Define the inputs of your model with a shape (Tx,)\n","    # Define s0 and c0, initial hidden state for the decoder LSTM of shape (n_s,)\n","    X = Input(shape=(Tx,human_vocab_size))\n","    s0 = Input(shape=(n_s,), name='s0')\n","    c0 = Input(shape=(n_s,), name='c0')\n","    s = s0\n","    c = c0\n","    # Initialize empty list of outputs\n","    outputs = []\n","    \n","    ### START CODE HERE ###\n","    \n","    # Step 1: Define your pre-attention Bi-LSTM. Remember to use return_sequences=True. (≈ 1 line)\n","    a = Bidirectional(LSTM(n_a, return_sequences=True))(X)\n","    \n","    # Step 2: Iterate for Ty steps\n","    for t in range(Ty):\n","    \n","        # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t (≈ 1 line)\n","        context = one_step_attention(a, s)\n","        \n","        # Step 2.B: Apply the post-attention LSTM cell to the \"context\" vector.\n","        # Don't forget to pass: initial_state = [hidden state, cell state] (≈ 1 line)\n","        s, _, c = post_activation_LSTM_cell(context, initial_state=[s, c])\n","        \n","        # Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM (≈ 1 line)\n","        out = output_layer(s)\n","        \n","        # Step 2.D: Append \"out\" to the \"outputs\" list (≈ 1 line)\n","        outputs.append(out)\n","    \n","    # Step 3: Create model instance taking three inputs and returning the list of outputs. (≈ 1 line)\n","    model = Model([X, s0, c0], outputs)\n","    \n","    ### END CODE HERE ###\n","    \n","    return model"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CegtA-e8JAVx","colab_type":"code","colab":{}},"cell_type":"code","source":["model = model(eng_sentence_length, spa_sentence_length,n_a,n_s, num_encoder_tokens, num_decoder_tokens)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"X5YNoIFSJbJ-","colab_type":"code","outputId":"9b1aa44d-1e37-4d29-df3b-5e5a57e70718","executionInfo":{"status":"ok","timestamp":1552772858471,"user_tz":0,"elapsed":600,"user":{"displayName":"Eoghan Keany","photoUrl":"","userId":"01744055321449573695"}},"colab":{"base_uri":"https://localhost:8080/","height":2261}},"cell_type":"code","source":["model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_7 (InputLayer)            (None, 5, 1881)      0                                            \n","__________________________________________________________________________________________________\n","s0 (InputLayer)                 (None, 128)          0                                            \n","__________________________________________________________________________________________________\n","bidirectional_3 (Bidirectional) (None, 5, 128)       996352      input_7[0][0]                    \n","__________________________________________________________________________________________________\n","repeat_vector_2 (RepeatVector)  (None, 5, 128)       0           s0[0][0]                         \n","                                                                 lstm_7[0][0]                     \n","                                                                 lstm_7[1][0]                     \n","                                                                 lstm_7[2][0]                     \n","                                                                 lstm_7[3][0]                     \n","                                                                 lstm_7[4][0]                     \n","                                                                 lstm_7[5][0]                     \n","                                                                 lstm_7[6][0]                     \n","                                                                 lstm_7[7][0]                     \n","                                                                 lstm_7[8][0]                     \n","__________________________________________________________________________________________________\n","concatenate_2 (Concatenate)     (None, 5, 256)       0           bidirectional_3[0][0]            \n","                                                                 repeat_vector_2[10][0]           \n","                                                                 bidirectional_3[0][0]            \n","                                                                 repeat_vector_2[11][0]           \n","                                                                 bidirectional_3[0][0]            \n","                                                                 repeat_vector_2[12][0]           \n","                                                                 bidirectional_3[0][0]            \n","                                                                 repeat_vector_2[13][0]           \n","                                                                 bidirectional_3[0][0]            \n","                                                                 repeat_vector_2[14][0]           \n","                                                                 bidirectional_3[0][0]            \n","                                                                 repeat_vector_2[15][0]           \n","                                                                 bidirectional_3[0][0]            \n","                                                                 repeat_vector_2[16][0]           \n","                                                                 bidirectional_3[0][0]            \n","                                                                 repeat_vector_2[17][0]           \n","                                                                 bidirectional_3[0][0]            \n","                                                                 repeat_vector_2[18][0]           \n","                                                                 bidirectional_3[0][0]            \n","                                                                 repeat_vector_2[19][0]           \n","__________________________________________________________________________________________________\n","dense_4 (Dense)                 (None, 5, 1)         257         concatenate_2[10][0]             \n","                                                                 concatenate_2[11][0]             \n","                                                                 concatenate_2[12][0]             \n","                                                                 concatenate_2[13][0]             \n","                                                                 concatenate_2[14][0]             \n","                                                                 concatenate_2[15][0]             \n","                                                                 concatenate_2[16][0]             \n","                                                                 concatenate_2[17][0]             \n","                                                                 concatenate_2[18][0]             \n","                                                                 concatenate_2[19][0]             \n","__________________________________________________________________________________________________\n","attention_weights (Activation)  (None, 5, 1)         0           dense_4[10][0]                   \n","                                                                 dense_4[11][0]                   \n","                                                                 dense_4[12][0]                   \n","                                                                 dense_4[13][0]                   \n","                                                                 dense_4[14][0]                   \n","                                                                 dense_4[15][0]                   \n","                                                                 dense_4[16][0]                   \n","                                                                 dense_4[17][0]                   \n","                                                                 dense_4[18][0]                   \n","                                                                 dense_4[19][0]                   \n","__________________________________________________________________________________________________\n","dot_2 (Dot)                     (None, 1, 128)       0           attention_weights[10][0]         \n","                                                                 bidirectional_3[0][0]            \n","                                                                 attention_weights[11][0]         \n","                                                                 bidirectional_3[0][0]            \n","                                                                 attention_weights[12][0]         \n","                                                                 bidirectional_3[0][0]            \n","                                                                 attention_weights[13][0]         \n","                                                                 bidirectional_3[0][0]            \n","                                                                 attention_weights[14][0]         \n","                                                                 bidirectional_3[0][0]            \n","                                                                 attention_weights[15][0]         \n","                                                                 bidirectional_3[0][0]            \n","                                                                 attention_weights[16][0]         \n","                                                                 bidirectional_3[0][0]            \n","                                                                 attention_weights[17][0]         \n","                                                                 bidirectional_3[0][0]            \n","                                                                 attention_weights[18][0]         \n","                                                                 bidirectional_3[0][0]            \n","                                                                 attention_weights[19][0]         \n","                                                                 bidirectional_3[0][0]            \n","__________________________________________________________________________________________________\n","c0 (InputLayer)                 (None, 128)          0                                            \n","__________________________________________________________________________________________________\n","lstm_7 (LSTM)                   [(None, 128), (None, 131584      dot_2[10][0]                     \n","                                                                 s0[0][0]                         \n","                                                                 c0[0][0]                         \n","                                                                 dot_2[11][0]                     \n","                                                                 lstm_7[0][0]                     \n","                                                                 lstm_7[0][2]                     \n","                                                                 dot_2[12][0]                     \n","                                                                 lstm_7[1][0]                     \n","                                                                 lstm_7[1][2]                     \n","                                                                 dot_2[13][0]                     \n","                                                                 lstm_7[2][0]                     \n","                                                                 lstm_7[2][2]                     \n","                                                                 dot_2[14][0]                     \n","                                                                 lstm_7[3][0]                     \n","                                                                 lstm_7[3][2]                     \n","                                                                 dot_2[15][0]                     \n","                                                                 lstm_7[4][0]                     \n","                                                                 lstm_7[4][2]                     \n","                                                                 dot_2[16][0]                     \n","                                                                 lstm_7[5][0]                     \n","                                                                 lstm_7[5][2]                     \n","                                                                 dot_2[17][0]                     \n","                                                                 lstm_7[6][0]                     \n","                                                                 lstm_7[6][2]                     \n","                                                                 dot_2[18][0]                     \n","                                                                 lstm_7[7][0]                     \n","                                                                 lstm_7[7][2]                     \n","                                                                 dot_2[19][0]                     \n","                                                                 lstm_7[8][0]                     \n","                                                                 lstm_7[8][2]                     \n","__________________________________________________________________________________________________\n","dense_6 (Dense)                 (None, 2936)         378744      lstm_7[0][0]                     \n","                                                                 lstm_7[1][0]                     \n","                                                                 lstm_7[2][0]                     \n","                                                                 lstm_7[3][0]                     \n","                                                                 lstm_7[4][0]                     \n","                                                                 lstm_7[5][0]                     \n","                                                                 lstm_7[6][0]                     \n","                                                                 lstm_7[7][0]                     \n","                                                                 lstm_7[8][0]                     \n","                                                                 lstm_7[9][0]                     \n","==================================================================================================\n","Total params: 1,506,937\n","Trainable params: 1,506,937\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"metadata":{"id":"potvQ_bvJlDw","colab_type":"code","colab":{}},"cell_type":"code","source":["#opt = Adam(lr = 0.005, beta_1=0.9, beta_2=0.999, decay = 0.01)\n","#model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n","model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"R4ChQ8ijJ5vf","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","input_words  = set([item for sublist in X for item in sublist])\n","target_words = set([item for sublist in Y for item in sublist])\n","num_encoder_tokens = len(input_words)\n","num_decoder_tokens = len(target_words)\n","\n","\n","max_encoder_seq_length = max([len(sample) for sample in X])\n","max_decoder_seq_length = max([len(sample) for sample in Y])\n","\n","\n","input_token_index = dict(\n","  [(char, i) for i, char in enumerate(input_words)])\n","target_token_index = dict(\n","  [(char, i) for i, char in enumerate(target_words)])\n","\n","encoder_input_data = np.zeros(\n","  (len(X), max_encoder_seq_length, num_encoder_tokens),\n","  dtype='float32')\n","decoder_input_data = np.zeros(\n","  (len(X), max_decoder_seq_length, num_decoder_tokens),\n","  dtype='float32')\n","decoder_target_data = np.zeros(\n","  (len(X), max_decoder_seq_length, num_decoder_tokens),\n","  dtype='float32')\n","\n","for i, (X, Y) in enumerate(zip(X, Y)):\n","  for t, char in enumerate(X):\n","    encoder_input_data[i, t, input_token_index[char]] = 1.\n","  for t, char in enumerate(Y):\n","    # decoder_target_data is ahead of decoder_input_data by one timestep\n","    decoder_input_data[i, t, target_token_index[char]] = 1.\n","    if t > 0:\n","      # decoder_target_data will be ahead by one timestep\n","      # and will not include the start character.\n","      decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n","\n","s0 = np.zeros((5308, n_s))\n","c0 = np.zeros((5308, n_s))\n","\n","outputs = list(decoder_input_data.swapaxes(0,1))\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CmOyBjUmLNkp","colab_type":"code","outputId":"abcbb6b8-9464-4168-9102-d2b989f23b6b","executionInfo":{"status":"error","timestamp":1552773075045,"user_tz":0,"elapsed":198412,"user":{"displayName":"Eoghan Keany","photoUrl":"","userId":"01744055321449573695"}},"colab":{"base_uri":"https://localhost:8080/","height":1118}},"cell_type":"code","source":["print(encoder_input_data.shape)\n","model.fit([encoder_input_data,s0,c0], outputs, epochs=20, batch_size=128)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(5308, 5, 1881)\n","Epoch 1/20\n","5308/5308 [==============================] - 36s 7ms/step - loss: 29.4845 - dense_6_loss: 1.7511e-04 - dense_6_acc: 0.1319 - dense_6_acc_1: 0.0000e+00 - dense_6_acc_2: 0.0599 - dense_6_acc_3: 0.3223 - dense_6_acc_4: 0.4050 - dense_6_acc_5: 0.1558 - dense_6_acc_6: 0.0260 - dense_6_acc_7: 0.0058 - dense_6_acc_8: 7.5358e-04 - dense_6_acc_9: 1.8839e-04\n","Epoch 2/20\n","5308/5308 [==============================] - 20s 4ms/step - loss: 20.6956 - dense_6_loss: 2.2180e-04 - dense_6_acc: 1.0000 - dense_6_acc_1: 0.0000e+00 - dense_6_acc_2: 0.0607 - dense_6_acc_3: 0.3318 - dense_6_acc_4: 0.4154 - dense_6_acc_5: 0.1590 - dense_6_acc_6: 0.0264 - dense_6_acc_7: 0.0058 - dense_6_acc_8: 7.5358e-04 - dense_6_acc_9: 1.8839e-04\n","Epoch 3/20\n","5308/5308 [==============================] - 20s 4ms/step - loss: 19.1619 - dense_6_loss: 1.3321e-04 - dense_6_acc: 1.0000 - dense_6_acc_1: 0.0121 - dense_6_acc_2: 0.0607 - dense_6_acc_3: 0.3318 - dense_6_acc_4: 0.4154 - dense_6_acc_5: 0.1590 - dense_6_acc_6: 0.0264 - dense_6_acc_7: 0.0058 - dense_6_acc_8: 7.5358e-04 - dense_6_acc_9: 1.8839e-04\n","Epoch 4/20\n","5308/5308 [==============================] - 20s 4ms/step - loss: 18.4465 - dense_6_loss: 5.1169e-05 - dense_6_acc: 1.0000 - dense_6_acc_1: 0.0914 - dense_6_acc_2: 0.0686 - dense_6_acc_3: 0.3318 - dense_6_acc_4: 0.4154 - dense_6_acc_5: 0.1590 - dense_6_acc_6: 0.0264 - dense_6_acc_7: 0.0058 - dense_6_acc_8: 7.5358e-04 - dense_6_acc_9: 1.8839e-04\n","Epoch 5/20\n","5308/5308 [==============================] - 20s 4ms/step - loss: 17.9616 - dense_6_loss: 3.4449e-05 - dense_6_acc: 1.0000 - dense_6_acc_1: 0.0942 - dense_6_acc_2: 0.1076 - dense_6_acc_3: 0.3325 - dense_6_acc_4: 0.4154 - dense_6_acc_5: 0.1590 - dense_6_acc_6: 0.0264 - dense_6_acc_7: 0.0058 - dense_6_acc_8: 7.5358e-04 - dense_6_acc_9: 1.8839e-04\n","Epoch 6/20\n","5308/5308 [==============================] - 20s 4ms/step - loss: 17.6057 - dense_6_loss: 3.3562e-05 - dense_6_acc: 1.0000 - dense_6_acc_1: 0.0942 - dense_6_acc_2: 0.1183 - dense_6_acc_3: 0.3327 - dense_6_acc_4: 0.4154 - dense_6_acc_5: 0.1590 - dense_6_acc_6: 0.0264 - dense_6_acc_7: 0.0058 - dense_6_acc_8: 7.5358e-04 - dense_6_acc_9: 1.8839e-04\n","Epoch 7/20\n","5308/5308 [==============================] - 20s 4ms/step - loss: 17.2657 - dense_6_loss: 2.8031e-05 - dense_6_acc: 1.0000 - dense_6_acc_1: 0.0946 - dense_6_acc_2: 0.1202 - dense_6_acc_3: 0.3372 - dense_6_acc_4: 0.4160 - dense_6_acc_5: 0.1590 - dense_6_acc_6: 0.0264 - dense_6_acc_7: 0.0058 - dense_6_acc_8: 7.5358e-04 - dense_6_acc_9: 1.8839e-04\n","Epoch 8/20\n","5308/5308 [==============================] - 20s 4ms/step - loss: 16.9596 - dense_6_loss: 1.0318e-04 - dense_6_acc: 1.0000 - dense_6_acc_1: 0.1134 - dense_6_acc_2: 0.1209 - dense_6_acc_3: 0.3380 - dense_6_acc_4: 0.4167 - dense_6_acc_5: 0.1590 - dense_6_acc_6: 0.0264 - dense_6_acc_7: 0.0058 - dense_6_acc_8: 7.5358e-04 - dense_6_acc_9: 1.8839e-04\n","Epoch 9/20\n","3072/5308 [================>.............] - ETA: 8s - loss: 16.7128 - dense_6_loss: 0.0000e+00 - dense_6_acc: 1.0000 - dense_6_acc_1: 0.1292 - dense_6_acc_2: 0.1217 - dense_6_acc_3: 0.3405 - dense_6_acc_4: 0.4150 - dense_6_acc_5: 0.1579 - dense_6_acc_6: 0.0251 - dense_6_acc_7: 0.0062 - dense_6_acc_8: 9.7656e-04 - dense_6_acc_9: 0.0000e+00"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-73-d4fce643e157>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_input_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoder_input_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"id":"GC9c5AqOQh4N","colab_type":"code","outputId":"8652b7d3-fb17-4346-8404-26c31e251b2f","executionInfo":{"status":"ok","timestamp":1552772758025,"user_tz":0,"elapsed":505,"user":{"displayName":"Eoghan Keany","photoUrl":"","userId":"01744055321449573695"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"cell_type":"code","source":["for ele in [30,22,35,500, 700, 800,5000,4500,3000,2000]:\n","  input_seq = encoder_input_data[ele: ele + 1]\n","  prediction = model.predict([input_seq, s0, c0])\n","  prediction = np.argmax(prediction, axis = -1)\n","  output = [reverse_target_char_index[int(i)] for i in prediction]\n","  print(output)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['blanca', 'mentiroso', 'monton', 'monton', 'monton', 'monton', 'monton', 'monton', 'monton', 'monton']\n","['blanca', 'dedico', 'monton', 'monton', 'monton', 'monton', 'monton', 'monton', 'monton', 'monton']\n","['blanca', 'monton', 'monton', 'monton', 'monton', 'monton', 'monton', 'monton', 'monton', 'monton']\n","['blanca', 'mentiroso', 'monton', 'monton', 'monton', 'monton', 'monton', 'monton', 'monton', 'monton']\n","['blanca', 'preguntare', 'murcielago', 'monton', 'monton', 'monton', 'monton', 'monton', 'monton', 'monton']\n","['blanca', 'caminando', 'monton', 'monton', 'monton', 'monton', 'monton', 'monton', 'monton', 'monton']\n","['blanca', 'impaciente', 'dedico', 'impaciente', 'monton', 'monton', 'monton', 'monton', 'monton', 'monton']\n","['blanca', 'dedico', 'monton', 'monton', 'monton', 'monton', 'monton', 'monton', 'monton', 'monton']\n","['blanca', 'preguntare', 'murcielago', 'monton', 'monton', 'monton', 'monton', 'monton', 'monton', 'monton']\n","['blanca', 'caminando', 'monton', 'monton', 'monton', 'monton', 'monton', 'monton', 'monton', 'monton']\n"],"name":"stdout"}]},{"metadata":{"id":"FOsoAHhzfOLN","colab_type":"code","outputId":"e1b09d2d-1a87-4533-8afe-de16629f8ae8","executionInfo":{"status":"ok","timestamp":1553119634360,"user_tz":0,"elapsed":1912,"user":{"displayName":"Eoghan Keany","photoUrl":"","userId":"01744055321449573695"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"cell_type":"code","source":["from keras.layers import SimpleRNN\n","\n","encoder_input = Input(shape=(eng_sentence_length,))\n","encoder = Embedding(num_encoder_tokens, 64, input_length=eng_sentence_length, mask_zero=True)(encoder_input)\n","encoder = LSTM(64, return_sequences=True, unroll=True)(encoder)\n","encoder_last = encoder[:,-1,:]\n","\n","print('encoder', encoder)\n","print('encoder_last', encoder_last)\n","\n","decoder_input = Input(shape=(spa_sentence_length,))\n","decoder = Embedding(num_decoder_tokens, 64, input_length=spa_sentence_length, mask_zero=True)(decoder_input)\n","decoder = LSTM(64, return_sequences=True, unroll=True)(decoder, initial_state=[encoder_last, encoder_last])\n","\n","print('decoder', decoder)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["encoder Tensor(\"lstm_3/transpose_2:0\", shape=(?, 5, 64), dtype=float32)\n","encoder_last Tensor(\"strided_slice:0\", shape=(?, 64), dtype=float32)\n","decoder Tensor(\"lstm_4/transpose_2:0\", shape=(?, 10, 64), dtype=float32)\n"],"name":"stdout"}]},{"metadata":{"id":"3B4bJWcTf_LW","colab_type":"code","outputId":"ff0f11e3-c3ae-4678-d0fb-2111900e2784","executionInfo":{"status":"ok","timestamp":1553119637541,"user_tz":0,"elapsed":806,"user":{"displayName":"Eoghan Keany","photoUrl":"","userId":"01744055321449573695"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"cell_type":"code","source":["from keras.layers import Activation, dot, concatenate\n","from keras.layers import Input, Embedding, LSTM, TimeDistributed, Dense\n","\n","# Equation (7) with 'dot' score from Section 3.1 in the paper.\n","# Note that we reuse Softmax-activation layer instead of writing tensor calculation\n","attention = dot([decoder, encoder], axes=[2, 2])\n","attention = Activation('softmax', name='attention')(attention)\n","print('attention', attention)\n","\n","context = dot([attention, encoder], axes=[2,1])\n","print('context', context)\n","\n","decoder_combined_context = concatenate([context, decoder])\n","print('decoder_combined_context', decoder_combined_context)\n","\n","# Has another weight + tanh layer as described in equation (5) of the paper\n","output = TimeDistributed(Dense(64, activation=\"tanh\"))(decoder_combined_context)\n","output = TimeDistributed(Dense(num_decoder_tokens, activation=\"softmax\"))(output)\n","print('output', output)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["attention Tensor(\"attention/truediv:0\", shape=(?, 10, 5), dtype=float32)\n","context Tensor(\"dot_2/MatMul:0\", shape=(?, 10, 64), dtype=float32)\n","decoder_combined_context Tensor(\"concatenate_1/concat:0\", shape=(?, 10, 128), dtype=float32)\n","output Tensor(\"time_distributed_2/Reshape_1:0\", shape=(?, 10, 2936), dtype=float32)\n"],"name":"stdout"}]},{"metadata":{"id":"fQPqWMHtggW0","colab_type":"code","outputId":"cb774b40-7502-48c7-9d85-8b85684440e1","executionInfo":{"status":"ok","timestamp":1553119642387,"user_tz":0,"elapsed":593,"user":{"displayName":"Eoghan Keany","photoUrl":"","userId":"01744055321449573695"}},"colab":{"base_uri":"https://localhost:8080/","height":595}},"cell_type":"code","source":["model = Model(inputs=[encoder_input, decoder_input], outputs=[output])\n","model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n","model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_6 (InputLayer)            (None, 10)           0                                            \n","__________________________________________________________________________________________________\n","input_5 (InputLayer)            (None, 5)            0                                            \n","__________________________________________________________________________________________________\n","embedding_5 (Embedding)         (None, 10, 64)       187904      input_6[0][0]                    \n","__________________________________________________________________________________________________\n","embedding_4 (Embedding)         (None, 5, 64)        120384      input_5[0][0]                    \n","__________________________________________________________________________________________________\n","lstm_4 (LSTM)                   (None, 10, 64)       33024       embedding_5[0][0]                \n","__________________________________________________________________________________________________\n","lstm_3 (LSTM)                   (None, 5, 64)        33024       embedding_4[0][0]                \n","__________________________________________________________________________________________________\n","dot_1 (Dot)                     (None, 10, 5)        0           lstm_4[0][0]                     \n","                                                                 lstm_3[0][0]                     \n","__________________________________________________________________________________________________\n","attention (Activation)          (None, 10, 5)        0           dot_1[0][0]                      \n","__________________________________________________________________________________________________\n","dot_2 (Dot)                     (None, 10, 64)       0           attention[0][0]                  \n","                                                                 lstm_3[0][0]                     \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 10, 128)      0           dot_2[0][0]                      \n","                                                                 lstm_4[0][0]                     \n","__________________________________________________________________________________________________\n","time_distributed_1 (TimeDistrib (None, 10, 64)       8256        concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","time_distributed_2 (TimeDistrib (None, 10, 2936)     190840      time_distributed_1[0][0]         \n","==================================================================================================\n","Total params: 573,432\n","Trainable params: 573,432\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"metadata":{"id":"MZDdFcE7gyUk","colab_type":"code","outputId":"37a6a0ea-a126-4b34-f3e6-26ba0985454a","executionInfo":{"status":"ok","timestamp":1553119937533,"user_tz":0,"elapsed":170814,"user":{"displayName":"Eoghan Keany","photoUrl":"","userId":"01744055321449573695"}},"colab":{"base_uri":"https://localhost:8080/","height":714}},"cell_type":"code","source":["model.fit(x = [encoder_input_data, decoder_input_data],y = [decoder_target_data] ,verbose=2, batch_size=128, epochs=20)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 1/20\n"," - 8s - loss: 2.7981\n","Epoch 2/20\n"," - 9s - loss: 2.7504\n","Epoch 3/20\n"," - 9s - loss: 2.7058\n","Epoch 4/20\n"," - 8s - loss: 2.6659\n","Epoch 5/20\n"," - 9s - loss: 2.6232\n","Epoch 6/20\n"," - 9s - loss: 2.5825\n","Epoch 7/20\n"," - 9s - loss: 2.5422\n","Epoch 8/20\n"," - 9s - loss: 2.5006\n","Epoch 9/20\n"," - 8s - loss: 2.4586\n","Epoch 10/20\n"," - 9s - loss: 2.4211\n","Epoch 11/20\n"," - 9s - loss: 2.3842\n","Epoch 12/20\n"," - 8s - loss: 2.3486\n","Epoch 13/20\n"," - 9s - loss: 2.3130\n","Epoch 14/20\n"," - 9s - loss: 2.2814\n","Epoch 15/20\n"," - 8s - loss: 2.2484\n","Epoch 16/20\n"," - 9s - loss: 2.2167\n","Epoch 17/20\n"," - 9s - loss: 2.1894\n","Epoch 18/20\n"," - 8s - loss: 2.1598\n","Epoch 19/20\n"," - 8s - loss: 2.1311\n","Epoch 20/20\n"," - 8s - loss: 2.1044\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f0ef0da1320>"]},"metadata":{"tags":[]},"execution_count":20}]},{"metadata":{"id":"ijlSDOvTF3UY","colab_type":"code","outputId":"90fc3e33-5873-4f83-f4ad-747786338907","executionInfo":{"status":"ok","timestamp":1553120129241,"user_tz":0,"elapsed":706,"user":{"displayName":"Eoghan Keany","photoUrl":"","userId":"01744055321449573695"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"cell_type":"code","source":["def generate(text):\n","    encoder_input = text\n","    decoder_input = np.zeros(shape=(eng_sentence_length, spa_sentence_length))\n","    decoder_input[:,0] = target_token_index['START_']\n","    for i in range(1, spa_sentence_length):\n","        output = model.predict([encoder_input, decoder_input]).argmax(axis=2)\n","        decoder_input[:,i] = output[:,i]\n","    return decoder_input[:,1:]\n","\n","def decode(sequence):\n","  text = ''\n","  for i in sequence:\n","    if reverse_target_char_index[i] == '_END':\n","      break\n","    else:\n","      text += ' ' + reverse_target_char_index[i]\n","    print(reverse_target_char_index[i])\n","    \n","  return text\n","\n","def to_katakana(text):\n","    decoder_output = generate(text)\n","    return decode(decoder_output[0])\n","\n","\n","to_katakana(encoder_input_data[500:501])\n","#decoder_input = np.zeros(shape=(eng_sentence_length, spa_sentence_length))\n","#decoder_input[:,0] = target_token_index['START_']\n","#model.predict([encoder_input_data[13:14], decoder_input]).argmax(axis=2)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tom\n","un\n","favor\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["' tom un favor'"]},"metadata":{"tags":[]},"execution_count":25}]},{"metadata":{"id":"izhT5vYtLgv0","colab_type":"code","colab":{}},"cell_type":"code","source":["https://github.com/wanasit/katakana/blob/master/notebooks/Attention-based%20Sequence-to-Sequence%20in%20Keras.ipynb\n","https://github.com/gowtham1997/machine_translation/blob/master/Neural%20machine%20translation%20with%20attention%20-%20v2.ipynb\n","https://github.com/naveenkumarg651/Neural_machine_translation-with-Attention/blob/master/attention_s2s_hindi.py"],"execution_count":0,"outputs":[]}]}